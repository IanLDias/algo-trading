{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23754694",
   "metadata": {},
   "source": [
    "Overview of papers in this field: https://arxiv.org/pdf/2106.00123.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130099f",
   "metadata": {},
   "source": [
    "## Critic-only Deep Reinforcement Learning (i.e. Deep Q Learning)\n",
    "\n",
    "https://jfds.pm-research.com/content/2/2/25\n",
    "\n",
    "The main critic method is DQN, but the main limitation is that it aims to solve a discrete action space problem. But prices for stocks are continuous-valued. Action state and space grows exponentially as you add more stocks/variables so can be a big limitation.\n",
    "\n",
    "- DRQN (RNN with DQN) in algo trading : https://arxiv.org/abs/2004.06627\n",
    "An agent has 3 actions: buy a share, sell a share, do nothing. Expected return is about 22-23% - more than a buy and hold strategy. \n",
    "\n",
    "\n",
    "- Another DRQN (in FX market): https://arxiv.org/abs/1807.02787v1. Uses more features than the previous paper, a small replay memory is used and enables the use of greedy policies as exploration is offset by extra feedback signals to the agent. Tested on FX, some currencies have a 60% annual return, with the average being 10%. \n",
    "\n",
    "- GDQN (GRU with DQN): https://www.sciencedirect.com/science/article/abs/pii/S0020025520304692. GRUs might be good for unknown parameters. Has a reward function that handles risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44634235",
   "metadata": {},
   "source": [
    "## Actor-only Deep Reinforcement Learning\n",
    "\n",
    "Has a continuous action space, but because of this it can take a longer time to train. \n",
    "\n",
    "- RNN, RL, fuzzy representations for input data. https://ieeexplore.ieee.org/document/7407387. Useful paper, but complicated. Try and recreate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c536065",
   "metadata": {},
   "source": [
    "## Actor-critic Deep Reinforcement Learning\n",
    "\n",
    "Examples are PPO, A2C.\n",
    "\n",
    "- Ensemble model with PPO, A2C, DDPG. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996. Features used are available balance: adjusted close price, shares already owned, MACD, RSI, CCI, ADX. Can buy/sell up to k shares. **Important paper**\n",
    "\n",
    "- Mixing sentiment with DRL: https://link.springer.com/chapter/10.1007/978-981-10-8201-6_5. Uses DDPG with RCNN for sentiment classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c445ee4",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- Daily timeframes may be suseptible to qualitative information (news, legal rulings, sentiment), while more granular timeframes may be better for an intelligent agent\n",
    "\n",
    "- Smaller lengths (2-3 months of data) would be more relevant for an actual trading algorithm instead of the 10+ years some models use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427f895",
   "metadata": {},
   "source": [
    "### Useful githubs\n",
    "https://github.com/pskrunner14/trading-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
